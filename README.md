# Blog Generator - Llama Local API

This project is a **Blog Generator** powered by a **locally hosted Llama model** using **LangChain** and **Streamlit**. The API enables users to generate high-quality blog content efficiently.

## Features
‚úÖ Generate blog posts using AI
‚úÖ Locally hosted Llama model for privacy
‚úÖ API-based interaction for automation
‚úÖ Streamlit UI for easy blog generation

## Requirements
Ensure you have the following installed:
- Python (>=3.8)
- Llama model (local)
- Required Python dependencies

## Installation

### 1Ô∏è‚É£ **Clone the Repository**
```bash
git clone <repository-url>
cd <repository-folder>
```

### 2Ô∏è‚É£ **Install Dependencies**
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ **Set Up Local Llama Model**
This project requires a locally hosted **Llama model** using **ctransformers**.

#### üîπ **Download the Model**
Download a compatible model from [TheBloke's Hugging Face](https://huggingface.co/TheBloke) or another source.

Example command:
```bash
mkdir models
wget -P models/ <model-download-link>
```

#### üîπ **Load the Model in Python**
Modify `app.py` to load your local Llama model:
```python
from ctransformers import CTransformers

llm = CTransformers(model='models/llama-2-7b-chat.ggmlv3.q8_0.bin',
                    model_type='llama',
                    config={'max_new_tokens': 256, 'temperature': 0.01})
```

{
    "response": "AI has been advancing rapidly, transforming industries..."
}
```

## Running the Streamlit App
To run the **Streamlit UI**, use:
```bash
streamlit run app.py
```

This will open a local UI where you can enter blog topics and generate content.

## Using `app.py` for Blog Generation
The `app.py` script includes a function that generates blog content using a locally hosted **Llama 2 model**.

### Function: `getLLamaresponse()`
This function takes in three parameters:
- **`input_text`** ‚Äì The topic of the blog
- **`no_words`** ‚Äì The desired word count
- **`blog_style`** ‚Äì The writing style of the blog

#### Example Usage:
```python
response = getLLamaresponse("AI in Healthcare", 500, "technical")
print(response)
```

The function uses **LangChain** to format a prompt and interacts with the **Llama 2 model** stored in `models/llama-2-7b-chat.ggmlv3.q8_0.bin`.

## Summary
‚úÖ **Generate AI-powered blogs locally**
‚úÖ **Secure and private** (no cloud API required)
‚úÖ **User-friendly UI with Streamlit**

---

Author: Chirag S Shetty

